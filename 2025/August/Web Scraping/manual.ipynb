{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10084fcc",
   "metadata": {},
   "source": [
    "# Python Web Scraping Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76baac",
   "metadata": {},
   "source": [
    "## Setup Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "pip install requests beautifulsoup4 lxml selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd80e9",
   "metadata": {},
   "source": [
    "- requests → Fetch web pages\n",
    "- BeautifulSoup → Parse HTML/XML\n",
    "- lxml → Faster parsing engine\n",
    "- selenium → For dynamic (JavaScript-heavy) sites\n",
    "- pandas → Store scraped tables/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d5d86",
   "metadata": {},
   "source": [
    "## Basic HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # avoid blocking\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.status_code)   # 200 = OK\n",
    "print(response.text[:500])    # preview HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a45565",
   "metadata": {},
   "source": [
    "## BeautifulSoup Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9270ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# Find by tag\n",
    "title = soup.title.text\n",
    "heading = soup.h1.text\n",
    "\n",
    "# Find first element\n",
    "div = soup.find(\"div\", {\"class\": \"content\"})\n",
    "\n",
    "# Find all elements\n",
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))\n",
    "\n",
    "# CSS selectors\n",
    "soup.select(\"div.article h2\")     # nested tags\n",
    "soup.select_one(\"span.price\")     # first match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87f19e",
   "metadata": {},
   "source": [
    "## Extract Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = soup.find(\"img\")\n",
    "print(img[\"src\"])       # image source\n",
    "print(img.get(\"alt\"))   # alt text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff2f83",
   "metadata": {},
   "source": [
    "## Handling Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377de010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = soup.find(\"table\")\n",
    "df = pd.read_html(str(table))[0]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c96cb",
   "metadata": {},
   "source": [
    "## Pagination Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, 6):\n",
    "    url = f\"https://example.com/page/{page}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    # parse as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25ed5c",
   "metadata": {},
   "source": [
    "## Selenium for Dynamic Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://example.com\")\n",
    "\n",
    "# Extract text\n",
    "element = driver.find_element(By.CLASS_NAME, \"price\")\n",
    "print(element.text)\n",
    "\n",
    "# Get multiple elements\n",
    "items = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "for item in items:\n",
    "    print(item.get_attribute(\"href\"))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df16c6",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# JSON\n",
    "import json\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(scraped_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f259e0",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cb5e4",
   "metadata": {},
   "source": [
    "✅ Respect robots.txt (`https://site.com/robots.txt`)\n",
    "✅ Use User-Agent headers to avoid blocking\n",
    "✅ Add `time.sleep()` to avoid rate limits\n",
    "✅ Store intermediate data (CSV/JSON)\n",
    "✅ Handle errors with `try/except`\n",
    "✅ For APIs, prefer requests + JSON over scraping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
